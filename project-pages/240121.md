# In-Distribution Self-Assessment with Large Language Models

In this project we explore the ability of **Large Language Models (LLMs)** to **self-assess whether they are operating within their training distribution** â€” in other words, how confidently and accurately a model can estimate its own capability to predict the next token given previous context.

We observe that LLMs tend to exhibit a meaningful correlation between their self-assessed confidence and their actual performance when in-distribution.  
Building on this insight, we design an **ensemble framework** that leverages this principle to improve overall model reliability and performance across diverse scenarios.

While the specific implementation details are beyond the scope of this summary, the project highlights the potential of self-evaluation as a mechanism for **adaptive reasoning** and **robust model behavior**.

We also note a more enhanced performance improvement for bigger models (we test 1B vs 2.5B parameter models) which was not obvious or expected.
